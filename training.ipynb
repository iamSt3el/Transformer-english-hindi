{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "import spacy\n",
    "import sys\n",
    "from indicnlp import common\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDIC_NPL_LIB_HOME = r\"indic_nlp_library\"\n",
    "INDIC_NLP_RESOURCES = r\"indic_nlp_resources\"\n",
    "\n",
    "sys.path.append(r'{}/src'.format(INDIC_NPL_LIB_HOME))\n",
    "common.set_resources_path(INDIC_NLP_RESOURCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        \"\"\" Dividing word's embedding into 'H' different heads\n",
    "            For ex: embed_size = 512 & heads = 8\n",
    "            Then 8 heads of 64 size are created\n",
    "        \"\"\"\n",
    "        assert (self.embed_size % self.heads == 0), \"Embed size should be in multiple of heads\"\n",
    "\n",
    "        self.values = nn.Linear(in_features=self.embed_size, out_features=self.embed_size, bias=False)\n",
    "        self.keys = nn.Linear(in_features=self.embed_size, out_features=self.embed_size, bias=False)\n",
    "        self.queries = nn.Linear(in_features=self.embed_size, out_features=self.embed_size, bias=False)\n",
    "        self.fc_out = nn.Linear(in_features=self.head_dim * self.heads, out_features=self.embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask=None):\n",
    "        N = query.shape[0]  # Number of training examples\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values = self.values(values)  # (N, value_len, heads, head_dim)\n",
    "        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n",
    "        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n",
    "\n",
    "        # Splitting embeddings into 'H' heads for creating multi-head attention\n",
    "        # V, K, Q reshape = num_samp, seq_len, heads, heads_dim\n",
    "\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        # Attention calculation\n",
    "        # attention score = softmax(Q*t(K))/sqrt(Q.shape[-1])\n",
    "        score = torch.einsum(\"nqhd,nkhd -> nhqk\", queries, keys)\n",
    "        \"\"\"\n",
    "        einsum explained: \"nqhd,nkhd -> nhqk\"\n",
    "        1. nqhd -> nhqd : queries.transpose(-2,-3) & nkhd -> nhkd : keys.transpose(-2,-3)\n",
    "        2. nhqk : (torch.bmm(nhqd.reshape(n*h,q,d), nhkd.reshape(n*h,k,d).transpose(-1,-2)).reshape(n,h,q,k)  \n",
    "        \"\"\"\n",
    "\n",
    "        if mask is not None:\n",
    "            \"\"\"\n",
    "            Masking is very critical for implementing decoder side self attention\n",
    "            Since in decoding side we want to have attention scores with previous time steps elements only\n",
    "            So for this we use upper triangular masked matrix \n",
    "            \"\"\"\n",
    "            score = score.masked_fill(mask == 0, float('-1e20'))\n",
    "        attention_score = torch.softmax(score / math.sqrt(self.head_dim), dim=-1)  # N, heads, query_len, key_len\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", attention_score, values).reshape(N, query_len, self.heads*self.head_dim)\n",
    "        # out.shape >> N, query_len, embed_size\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size=embed_size, heads=heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)  # Normalization for each example for each embed dim across seq_len\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_fwd = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion*embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask=None):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attention + query))  # Layernorm1 + Skip connection\n",
    "        forward = self.feed_fwd(x)\n",
    "        out = self.dropout(self.norm2(forward + x))  # Layernorm2 + Skip connection\n",
    "        return out\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    PE(pos,2i) = sin(pos/10000^(2i/emb_size))\n",
    "    PE(cos,2i+1) = cos(pos/10000^(2i+1/emb_size))\n",
    "    \"\"\"\n",
    "    def __init__(self, max_len, embed_size, dropout, device):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, embed_size)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # column data : [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2) * -(math.log(10000.0) / embed_size))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # even place in emb_dim get sin wavelength\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # odd place in emb_dim get cos wavelength\n",
    "        pe = pe.unsqueeze(0).to(device)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.transformer_block = TransformerBlock(embed_size=embed_size,\n",
    "                                                  heads=heads,\n",
    "                                                  dropout=dropout,\n",
    "                                                  forward_expansion=forward_expansion)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, trg_mask, src_mask=None):\n",
    "        attention = self.attention(x, x, x, mask=trg_mask)\n",
    "        query = self.dropout(self.norm(attention + x))  # LayerNorm + Skip connection\n",
    "        out = self.transformer_block(value=value, key=key, query=query, mask=src_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_len):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.position_embedding = PositionalEncoding(max_len, embed_size, dropout, device=self.device)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embed_size=embed_size,\n",
    "                                 heads=heads,\n",
    "                                 dropout=dropout,\n",
    "                                 forward_expansion=forward_expansion)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.word_embedding(x)\n",
    "        out = self.position_embedding(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(value=out, key=out, query=out, mask=mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, trg_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_len):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = PositionalEncoding(max_len, embed_size, dropout, device=self.device)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_size=embed_size,\n",
    "                             heads=heads,\n",
    "                             dropout=dropout,\n",
    "                             forward_expansion=forward_expansion,\n",
    "                             device=self.device)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "\n",
    "    def forward(self, x, enc_out, trg_mask, src_mask=None):\n",
    "        x = self.word_embedding(x)\n",
    "        x = self.position_embedding(x)\n",
    "\n",
    "        \"\"\"\n",
    "        In decoder part key & value comes from the encoder output \n",
    "        while query comes from the self attention layer's output of the decoder         \n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x=x, value=enc_out, key=enc_out, trg_mask=trg_mask, src_mask=src_mask)\n",
    "\n",
    "        dec_out = self.fc_out(x)\n",
    "        return dec_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 src_vocab_size,\n",
    "                 trg_vocab_size,\n",
    "                 src_pad_idx,\n",
    "                 trg_pad_idx,\n",
    "                 embed_size,\n",
    "                 num_layers,\n",
    "                 forward_expansion,\n",
    "                 heads,\n",
    "                 dropout,\n",
    "                 device=\"cuda\",\n",
    "                 max_len=500):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(src_vocab_size=src_vocab_size,\n",
    "                               embed_size=embed_size,\n",
    "                               num_layers=num_layers,\n",
    "                               heads=heads,\n",
    "                               device=device,\n",
    "                               forward_expansion=forward_expansion,\n",
    "                               dropout=dropout,\n",
    "                               max_len=max_len)\n",
    "\n",
    "        self.decoder = Decoder(trg_vocab_size=trg_vocab_size,\n",
    "                               embed_size=embed_size,\n",
    "                               num_layers=num_layers,\n",
    "                               heads=heads,\n",
    "                               device=device,\n",
    "                               forward_expansion=forward_expansion,\n",
    "                               dropout=dropout,\n",
    "                               max_len=max_len)\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len)))\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src=src)\n",
    "        trg_mask = self.make_trg_mask(trg=trg)\n",
    "        enc_src = self.encoder(x=src, mask=src_mask)\n",
    "        out = self.decoder(x=trg, enc_out=enc_src, trg_mask=trg_mask, src_mask=src_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_eng = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_eng(text):\n",
    "    return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "def tokenize_hindi(text):\n",
    "    return [tok for tok in indic_tokenize.trivial_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defin field\n",
    "english_txt = Field(tokenize=tokenize_eng, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "hindi_txt = Field(tokenize=tokenize_hindi, init_token=\"<sos>\", eos_token=\"<eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fields = [('eng_text', english_txt), ('hindi_text', hindi_txt)]\n",
    "train_dt, val_dt = TabularDataset.splits(path='./', train='train_sm.csv', validation='val_sm.csv', format='csv', fields=data_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_txt.build_vocab(train_dt, max_size=10000, min_freq = 2)\n",
    "hindi_txt.build_vocab(train_dt, max_size=10000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "save_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 3e-4\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Iterator\n",
    "train_iter = BucketIterator(train_dt, batch_size=batch_size, sort_key=lambda x: len(x.eng_text), shuffle=True)\n",
    "val_iter = BucketIterator(val_dt, batch_size=batch_size, sort_key=lambda x: len(x.eng_text), shuffle=True)\n",
    "\n",
    "# Model hyper-parameters\n",
    "src_vocab_size = len(english_txt.vocab)\n",
    "trg_vocab_size = len(hindi_txt.vocab)\n",
    "embedding_size = 512\n",
    "num_heads = 8\n",
    "num_layers = 3\n",
    "dropout = 0.10\n",
    "max_len = 10000\n",
    "forward_expansion = 4\n",
    "src_pad_idx = english_txt.vocab.stoi[\"<pad>\"]\n",
    "trg_pad_idx = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steel/projects/machineTranslation-3/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# Defining model & optimizer attributes\n",
    "model = Transformer(src_vocab_size=src_vocab_size,\n",
    "                    trg_vocab_size=trg_vocab_size,\n",
    "                    src_pad_idx=src_pad_idx,\n",
    "                    trg_pad_idx=trg_pad_idx,\n",
    "                    embed_size=embedding_size,\n",
    "                    num_layers=num_layers,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                    heads=num_heads,\n",
    "                    dropout=dropout,\n",
    "                    device=device,\n",
    "                    max_len=max_len).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "pad_idx = hindi_txt.vocab.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "loss_tracker = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1641 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 44/1641 [00:05<03:24,  7.82it/s, gpu_allocated_mem=759.47705078125, gpu_reserved_mem=1280.0, loss=6.19, total_gpu_mem=3720.9375]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m loop \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_iter), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_iter))\n\u001b[0;32m----> 5\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Get input and targets and move to GPU if available\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Switching axis because bucket-iterator gives output of size(seq_len,bs)\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43minp_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meng_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhindi_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/machineTranslation-3/venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/projects/machineTranslation-3/venv/lib/python3.12/site-packages/torchtext/data/iterator.py:156\u001b[0m, in \u001b[0;36mIterator.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m             minibatch\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_key, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminibatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepeat:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/machineTranslation-3/venv/lib/python3.12/site-packages/torchtext/data/batch.py:34\u001b[0m, in \u001b[0;36mBatch.__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mgetattr\u001b[39m(x, name) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, \u001b[43mfield\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/projects/machineTranslation-3/venv/lib/python3.12/site-packages/torchtext/data/field.py:237\u001b[0m, in \u001b[0;36mField.process\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Process a list of examples to create a torch.Tensor.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03mPad, numericalize, and postprocess a batch and create a tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03m    and custom postprocessing Pipeline.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m padded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad(batch)\n\u001b[0;32m--> 237\u001b[0m tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumericalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m~/projects/machineTranslation-3/venv/lib/python3.12/site-packages/torchtext/data/field.py:359\u001b[0m, in \u001b[0;36mField.numericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocessing \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    357\u001b[0m         arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocessing(arr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 359\u001b[0m var \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequential \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first:\n\u001b[1;32m    362\u001b[0m     var\u001b[38;5;241m.\u001b[39mt_()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    loop = tqdm(enumerate(train_iter), total=len(train_iter))\n",
    "    for batch_idx, batch in loop:\n",
    "        # Get input and targets and move to GPU if available\n",
    "        # Switching axis because bucket-iterator gives output of size(seq_len,bs)\n",
    "        inp_data = batch.eng_text.permute(-1, -2).to(device)\n",
    "        target = batch.hindi_text.permute(-1, -2).to(device)\n",
    "\n",
    "        # Forward prop\n",
    "        output = model(inp_data, target[:, :-1])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output.reshape(-1, trg_vocab_size), target[:, 1:].reshape(-1))\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Checking GPU uses\n",
    "        if device.type == \"cuda\":\n",
    "            total_mem = torch.cuda.get_device_properties(0).total_memory/1024/1024\n",
    "            allocated_mem = torch.cuda.memory_allocated(0)/1024/1024\n",
    "            reserved_mem = torch.cuda.memory_reserved(0)/1024/1024\n",
    "        else:\n",
    "            total_mem = 0\n",
    "            allocated_mem = 0\n",
    "            reserved_mem = 0\n",
    "\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "\n",
    "        # Clipping exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        loop.set_postfix(loss=loss.item(), total_gpu_mem=str(total_mem), gpu_allocated_mem=str(allocated_mem), gpu_reserved_mem=str(reserved_mem))\n",
    "\n",
    "    train_mean_loss = sum(losses) / len(losses)\n",
    "    scheduler.step(train_mean_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for val_batch_idx, val_batch in tqdm(enumerate(val_iter), total=len(val_iter)):\n",
    "            val_inp_data = val_batch.eng_text.permute(-1, -2).to(device)\n",
    "            val_target = val_batch.hindi_text.permute(-1, -2).to(device)\n",
    "            val_output = model(val_inp_data, val_target[:, :-1])\n",
    "            val_loss = criterion(val_output.reshape(-1, trg_vocab_size), val_target[:, 1:].reshape(-1))\n",
    "            val_losses.append(val_loss.item())\n",
    "        val_mean_loss = sum(val_losses)/len(val_losses)\n",
    "\n",
    "    loss_tracker.append(val_mean_loss)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        if save_model and val_mean_loss == np.min(loss_tracker):\n",
    "            checkpoint = {\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "            }\n",
    "            save_checkpoint(checkpoint)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]: train_loss= {train_mean_loss}; val_loss= {val_mean_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully and ready for inference!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def load_model_for_inference(model_class, model_path, device, **model_params):\n",
    "    \"\"\"\n",
    "    Load a saved PyTorch model for inference.\n",
    "\n",
    "    Args:\n",
    "    model_class (torch.nn.Module): The class of the model to be loaded.\n",
    "    model_path (str): Path to the saved checkpoint file.\n",
    "    device (torch.device): The device to load the model onto (CPU or GPU).\n",
    "    **model_params: Additional parameters required to initialize the model.\n",
    "\n",
    "    Returns:\n",
    "    torch.nn.Module: The loaded model set to evaluation mode.\n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    model = model_class(**model_params).to(device)\n",
    "\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "    # Load the state dict into the model\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = load_model_for_inference(\n",
    "        model_class=Transformer,\n",
    "        model_path='my_checkpoint.pth.tar',\n",
    "        device=device,\n",
    "        src_vocab_size=src_vocab_size,\n",
    "        trg_vocab_size=trg_vocab_size,\n",
    "        src_pad_idx=src_pad_idx,\n",
    "        trg_pad_idx=trg_pad_idx,\n",
    "        embed_size=embedding_size,\n",
    "        num_layers=num_layers,\n",
    "        forward_expansion=forward_expansion,\n",
    "        heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    print(\"Model loaded successfully and ready for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "def beam_search(sentence, model, src_field, src_tokenizer, trg_field, trg_vcb_sz, k, max_ts=50, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "    # Ensure model is on the correct device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Tokenize the input sentence\n",
    "    sentence_tok = src_tokenizer(sentence)\n",
    "\n",
    "    # Add <sos> and <eos> in beginning and end respectively\n",
    "    sentence_tok.insert(0, src_field.init_token)\n",
    "    sentence_tok.append(src_field.eos_token)\n",
    "\n",
    "    # Converting text to indices\n",
    "    src_tok = torch.tensor([src_field.vocab.stoi[token] for token in sentence_tok], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    trg_tok = torch.tensor([trg_field.vocab.stoi[trg_field.init_token]], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # Setting 'eos' flag for target sentence\n",
    "    eos = trg_field.vocab.stoi[trg_field.eos_token]\n",
    "\n",
    "    # Store for top 'k' translations\n",
    "    trans_store = {}\n",
    "\n",
    "    store_seq_id = None\n",
    "    store_seq_prob = None\n",
    "    for ts in range(max_ts):\n",
    "        if ts == 0:\n",
    "            with torch.no_grad():\n",
    "                out = model(src_tok, trg_tok)  # [1, trg_vcb_sz]\n",
    "            topk = torch.topk(torch.log(torch.softmax(out, dim=-1)), dim=-1, k=k)\n",
    "            seq_id = torch.empty(size=(k, ts + 2), dtype=torch.long, device=device)\n",
    "            seq_id[:, :ts + 1] = trg_tok\n",
    "            seq_id[:, ts + 1] = topk.indices\n",
    "            seq_prob = topk.values.squeeze()\n",
    "            \n",
    "            if eos in seq_id[:, ts + 1]:\n",
    "                trans_store[seq_prob[seq_id[:, ts + 1] == eos].squeeze().item()] = seq_id[seq_id[:, ts + 1] == eos, :].squeeze()\n",
    "                store_seq_id = seq_id[seq_id[:, ts + 1] != eos, :].clone().to(device)\n",
    "                store_seq_prob = seq_prob[seq_id[:, ts + 1] != eos].clone().to(device)\n",
    "            else:\n",
    "                store_seq_id = seq_id.clone().to(device)\n",
    "                store_seq_prob = seq_prob.clone().to(device)\n",
    "        else:\n",
    "            src_tok = src_tok.squeeze()\n",
    "            src = src_tok.expand(size=(store_seq_id.shape[-2], len(src_tok))).to(device)\n",
    "            with torch.no_grad():\n",
    "                out = model(src, store_seq_id)\n",
    "            out = torch.log(torch.softmax(out[:, -1, :], dim=-1))  # [k, trg_vcb_sz]\n",
    "            all_comb = (store_seq_prob.view(-1, 1) + out).view(-1)\n",
    "            all_comb_idx = torch.tensor([(x, y) for x in range(store_seq_id.shape[-2]) for y in range(trg_vcb_sz)], device=device)\n",
    "            topk = torch.topk(all_comb, dim=-1, k=k)\n",
    "            top_seq_id = all_comb_idx[topk.indices.squeeze()]\n",
    "            top_seq_prob = topk.values\n",
    "            seq_id = torch.empty(size=(k, ts + 2), dtype=torch.long, device=device)\n",
    "            seq_id[:, :ts + 1] = torch.tensor([store_seq_id[i.tolist()].tolist() for i, y in top_seq_id], device=device)\n",
    "            seq_id[:, ts + 1] = torch.tensor([y.tolist() for i, y in top_seq_id], device=device)\n",
    "            seq_prob = top_seq_prob\n",
    "            \n",
    "            if eos in seq_id[:, ts + 1]:\n",
    "                for i, prob in enumerate(seq_prob[seq_id[:, ts + 1] == eos]):\n",
    "                    trans_store[prob.item()] = seq_id[seq_id[:, ts + 1] == eos][i].cpu()\n",
    "                store_seq_id = seq_id[seq_id[:, ts + 1] != eos, :].clone().to(device)\n",
    "                store_seq_prob = seq_prob[seq_id[:, ts + 1] != eos].clone().to(device)\n",
    "            else:\n",
    "                store_seq_id = seq_id.clone().to(device)\n",
    "                store_seq_prob = seq_prob.clone().to(device)\n",
    "        \n",
    "        if len(trans_store) == k:\n",
    "            break\n",
    "\n",
    "    if len(trans_store) == 0:\n",
    "        best_translation = store_seq_id[0].cpu()\n",
    "    else:\n",
    "        best_translation = trans_store[max(trans_store)]\n",
    "    \n",
    "    return \" \".join([trg_field.vocab.itos[w] for w in best_translation[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "आप कैसे हो ? <eos>\n"
     ]
    }
   ],
   "source": [
    "sentence = \"How are you?\"\n",
    "src_field = english_txt\n",
    "src_tokenizer = tokenize_eng\n",
    "trg_field = hindi_txt\n",
    "trg_vcb_sz = 10000\n",
    "\n",
    "tr = beam_search(sentence=sentence, model=model, src_field=src_field,src_tokenizer=src_tokenizer, trg_field=hindi_txt, trg_vcb_sz=trg_vcb_sz, k=5)\n",
    "print(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def tokenize(text, language='en'):\n",
    "    if isinstance(text, list):\n",
    "        return text  # Already tokenized\n",
    "    if language == 'en':\n",
    "        # Simple English tokenization\n",
    "        return text.lower().split()\n",
    "    elif language == 'hi':\n",
    "        # Simple Hindi tokenization\n",
    "        return re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
    "    else:\n",
    "        # Default to simple whitespace tokenization\n",
    "        return text.split()\n",
    "\n",
    "def calculate_bleu(reference, candidate, max_n=4, language='en'):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for a single sentence.\n",
    "    \n",
    "    :param reference: Reference translation (string or list of tokens)\n",
    "    :param candidate: Candidate translation (string or list of tokens)\n",
    "    :param max_n: Maximum n-gram size to consider (default: 4)\n",
    "    :param language: Language of the translations ('en' for English, 'hi' for Hindi, etc.)\n",
    "    :return: BLEU score\n",
    "    \"\"\"\n",
    "    reference_tokens = tokenize(reference, language)\n",
    "    candidate_tokens = tokenize(candidate, language)\n",
    "\n",
    "    def ngrams(tokens, n):\n",
    "        return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "    if len(candidate_tokens) == 0:\n",
    "        return 0\n",
    "\n",
    "    candidate_len = len(candidate_tokens)\n",
    "    reference_len = len(reference_tokens)\n",
    "\n",
    "    # Calculate brevity penalty\n",
    "    if candidate_len > reference_len:\n",
    "        bp = 1\n",
    "    else:\n",
    "        bp = math.exp(1 - reference_len / candidate_len)\n",
    "\n",
    "    # Calculate n-gram precisions\n",
    "    precisions = []\n",
    "    for n in range(1, min(max_n, candidate_len) + 1):\n",
    "        candidate_ngrams = Counter(ngrams(candidate_tokens, n))\n",
    "        reference_ngrams = Counter(ngrams(reference_tokens, n))\n",
    "        \n",
    "        matches = sum(min(candidate_ngrams[ngram], reference_ngrams[ngram]) for ngram in candidate_ngrams)\n",
    "        total = sum(candidate_ngrams.values())\n",
    "        \n",
    "        if total > 0:\n",
    "            precisions.append(matches / total)\n",
    "        else:\n",
    "            precisions.append(0)\n",
    "\n",
    "    # Calculate geometric mean of precisions\n",
    "    if all(p > 0 for p in precisions):\n",
    "        geo_mean = math.exp(sum(math.log(p) for p in precisions) / len(precisions))\n",
    "    else:\n",
    "        geo_mean = 0\n",
    "\n",
    "    return bp * geo_mean\n",
    "\n",
    "def calculate_corpus_bleu(references, candidates, language='en'):\n",
    "    \"\"\"\n",
    "    Calculate corpus-level BLEU score for multiple sentences.\n",
    "    \n",
    "    :param references: List of reference translations (strings or lists of tokens)\n",
    "    :param candidates: List of candidate translations (strings or lists of tokens)\n",
    "    :param language: Language of the translations\n",
    "    :return: Corpus-level BLEU score\n",
    "    \"\"\"\n",
    "    total_score = 0\n",
    "    for ref, cand in zip(references, candidates):\n",
    "        total_score += calculate_bleu(ref, cand, language=language)\n",
    "    return total_score / len(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I love to eat pizza.\n",
      "Translation: मुझे <unk> फोन हमारा पसंद है । .\n",
      "Reference: मुझे पिज्जा खाना पसंद है।\n",
      "BLEU score: 0.4109080290971358\n"
     ]
    }
   ],
   "source": [
    "def translate_and_evaluate(sentence, reference, model, src_field, src_tokenizer, trg_field, trg_vcb_sz, k, max_ts=50, target_language='hi'):\n",
    "    # Perform beam search\n",
    "    translation = beam_search(sentence, model, src_field, src_tokenizer, trg_field, trg_vcb_sz, k, max_ts)\n",
    "    \n",
    "    # Remove <eos> token if present\n",
    "    translation = translation.replace(\" <eos>\", \"\").strip()\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    bleu_score = calculate_bleu(reference, translation, language=target_language)\n",
    "    \n",
    "    return translation, bleu_score\n",
    "\n",
    "# Example usage\n",
    "sentence = \"I love to eat pizza.\"\n",
    "reference = \"मुझे पिज्जा खाना पसंद है।\"  # Hindi reference\n",
    "translation, bleu_score = translate_and_evaluate(sentence, reference, model, src_field, src_tokenizer, trg_field, trg_vcb_sz, k=5, target_language='hi')\n",
    "print(f\"Input: {sentence}\")\n",
    "print(f\"Translation: {translation}\")\n",
    "print(f\"Reference: {reference}\")\n",
    "print(f\"BLEU score: {bleu_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
